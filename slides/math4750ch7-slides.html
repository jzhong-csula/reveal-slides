<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>MATH 4750 - Introduction to Mathematical Statistics</title>
<meta name="author" content="\\
Jie Zhong \\
Department of Mathematics \\
California State University, Los Angeles"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="../dist/reveal.css"/>

<link rel="stylesheet" href="../dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="../reveal.js-plugins/chalkboard/style.css"/>

<link rel="stylesheet" href="../reveal.js-plugins/menu/font-awesome/css/fontawesome.css"/>

<link rel="stylesheet" href="../gnohz.css"/>
<script>window.MathJax = { TeX: {Macros: {range: "\\text{Range}", ow: "\\text{otherwise}"}} }</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1>MATH 4750 - Introduction to Mathematical Statistics</h1><h2></h2><h6> <br />
Jie Zhong <br />
Department of Mathematics <br />
California State University, Los Angeles</h6>
</section>

<section>
<section id="slide-orgb89733a">
<h2 id="orgb89733a">Chapter 7 Estimation</h2>
<div class="outline-text-2" id="text-orgb89733a">
</div>
</section>
</section>
<section>
<section id="slide-org1e00275">
<h3 id="org1e00275">7.1 Statistical Inference</h3>
<div class="outline-text-3" id="text-org1e00275">
</div>
</section>
</section>
<section>
<section id="slide-org2c75ac9">
<h4 id="org2c75ac9">Example 7.1.1 (Lifetimes of Electronic Components)</h4>
<p data-fragment-index="1" class="fragment">
<span style="color: rgb(24,116,205)">Goal</span>: know as much as possible about how long each component is likely to last
</p>
<ul>
<li data-fragment-index="2" class="fragment">Collect data: \(X_1, X_2, \dots\) (observations)</li>
<li data-fragment-index="3" class="fragment">Choose the family of exponential distributions to model the length of time</li>
<li data-fragment-index="4" class="fragment">If the failure rate \(\theta\) is known, then \(X_i \sim \text{Exp}(\theta)\)</li>
<li data-fragment-index="5" class="fragment">We are also interested \(\theta\) itself because \(\mathbb{E}(X_i) = 1/\theta\)</li>

</ul>

</section>
</section>
<section>
<section id="slide-orga199fcc">
<h4 id="orga199fcc">How to Estimate \(\theta\)?</h4>
<p class="fragment">
By LLN,
</p>
<div class="fragment">
\begin{align*}
  \frac{1}{n} \sum_{i=1}^n X_i \xrightarrow{\mathbb{P}} \frac{1}{\theta},
\end{align*}

</div>

<p class="fragment">
and Theorem 6.2.5 says
</p>
<div class="fragment">
\begin{align*}
  \frac{n}{\sum_{i=1}^n X_i} \xrightarrow{\mathbb{P}} \theta.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-orgf612622">
<h4 id="orgf612622">Statistical Model</h4>
<ul>
<li data-fragment-index="1" class="fragment">Identification of random variable of interest</li>
<li data-fragment-index="2" class="fragment">Specification of a joint distribution</li>
<li data-fragment-index="3" class="fragment">Identification of unknown parameters, and their distributions</li>

</ul>

<p data-fragment-index="4" class="fragment">
<span style="color: rgb(24,116,205)">Note</span>: When we treat the unknown parameter(s) \(\theta\) as random, then the joint distribution of the observable random variables indexed by \(\theta\) is understood as the <b>conditional distribution</b> of the observable
random variables given \(\theta\).
</p>

</section>
</section>
<section>
<section id="slide-orge0fce65">
<h4 id="orge0fce65">Definitions</h4>
<p class="fragment">
<span style="color: rgb(24,116,205)">Statistical inference</span>:
</p>
<p class="fragment">
A procedure that produce a probabilistic statement about some or all parts of a statistical model.
</p>

<p class="fragment">
In a problem of statistical inference, a characteristic or combination of characteristics that determine the joint distribution for the random variables of interest is called a <i>parameter</i> of the distribution. The set \(\Omega\) of all possible values of a parameter \(\theta\) or of a vector of parameters \((\theta_1, \theta_2, \dots, \theta_k)\) is called the
<i>parameter space</i>.
</p>

</section>
</section>
<section>
<section id="slide-orgf967a94">
<h4 id="orgf967a94">Statistic</h4>
<p class="fragment">
Suppose \(X_1, X_2, \dots, X_n\) are observable random variables. Let \(r\) be an arbitrary real-valued function of \(n\) real variables. Then the random variable \(T = r(X_1, X_2, \dots, X_n)\) is called a <i>statistic</i>.
</p>

<p class="fragment">
For example,
</p>
<div class="fragment">
\begin{align*}
  \overline{X}_n & = \frac{1}{n} \sum_{i=1}^n X_i\\
Y_n & = \max\{ X_1, \dots, X_n\} ~\text{or} ~ \min\{X_1, \dots, X_n\}\\
C_n & = 3
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-orgb88b36e">
<h4 id="orgb88b36e">General Classes of Inference Problems</h4>
<ul>
<li>Prediction (Chapter 7 and 8)</li>
<li>Statistical decision problems (Chapter 9)</li>
<li>Experimental design</li>

</ul>

</section>
</section>
<section>
<section id="slide-org27a4107">
<h3 id="org27a4107">7.2 Prior and Posterior Distributions</h3>
<div class="outline-text-3" id="text-org27a4107">
</div>
</section>
</section>
<section>
<section id="slide-org5a53bd5">
<h4 id="org5a53bd5">Example 7.1.1 (Revisited)</h4>
<p>
Suppose before observing any data, the company believes the failure rate is around \(0.5\) per year but not sure.
</p>
<p class="fragment">
They model \(\theta\) as a random variable which has the exponential distribution, i.e. \(\theta \sim \text{Exp}(0.5)\).
</p>

</section>
</section>
<section>
<section id="slide-orgb5e62ec">
<h4 id="orgb5e62ec">Definition</h4>
<p class="fragment">
Suppose one has a statistical model with parameter \(\theta\). If one treats \(\theta\) as a random variable, then the distribution that one assigns to \(\theta\) <b>before</b> observing any other random variables of interest is called its <i>prior distribution</i>.
</p>

<p class="fragment">
We use \(\xi(\theta)\) to denote the prior p.m.f. or p.d.f. as a function of \(\theta\).
</p>

</section>
</section>
<section>
<section id="slide-org1c1b155">
<h4 id="org1c1b155">Example 7.2.2</h4>
<p>
Let \(\theta\) denote the probability of obtaining a head when a certain coin is tossed, and suppose that it is known that the coin either is fair or has a head on each side: \(\theta =1/2\) or \(\theta = 1\).
</p>

<p class="fragment">
If the prior probability that the coin is fair is \(0.8\), then the prior p.m.f. of \(\theta\) is
</p>
<div class="fragment">
\begin{align*}
  \xi(1/2)  = 0.8, \quad \xi(1) = 0.2.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-orga1463a0">
<h4 id="orga1463a0">Example 7.2.3</h4>
<p>
Suppose that the proportion \(\theta\) of defective items in a large manufactured lot is unknown and that the prior distribution assigned to \(\theta\) is the uniform distribution on the interval \([0, 1]\).
</p>

<p class="fragment">
Then the prior p.d.f. of \(\theta\) is
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta) = \begin{cases}
1, & \mbox{for } 0 < \theta < 1,\\
0, & \mbox{otherwise.}
\end{cases}
\end{align*}

</div>
</section>
</section>
<section>
<section id="slide-org5b56406">
<h4 id="org5b56406">Example 7.2.4</h4>
<p>
Lifetimes of Fluorescent Lamps
</p>
<ul>
<li data-fragment-index="1" class="fragment">Suppose the lifetimes (in hours)  of fluorescent lamps are to be observed.</li>
<li data-fragment-index="2" class="fragment">Assume that the lifetime \(\sim \text{Exp}(\theta)\).</li>
<li data-fragment-index="3" class="fragment">Prior distribution of \(\theta\): gamma distribution with \(\mu =0.0002\) and \(\sigma = 0.0001\).</li>

</ul>

<p data-fragment-index="4" class="fragment 4">
Determine the prior p.d.f. of \(\theta\).
</p>

</section>
</section>
<section>
<section id="slide-org494babc">
<h4 id="org494babc">Example 7.2.4 - Continued</h4>
<p>
<i>Solution</i>
</p>
<p class="fragment">
Recall that if \(\theta \sim \text{Gamma} (\alpha, \lambda)\), then
</p>
<div class="fragment">
\begin{align*}
  \mathbb{E}(\theta) = \frac{\alpha}{\lambda}, \quad \text{Var} (\theta) = \frac{\alpha}{\lambda^2}.
\end{align*}

</div>
<p class="fragment">
Therefore, we have
</p>
<div class="fragment">
\begin{align*}
  \frac{\alpha}{\lambda} = 0.0002, \quad \frac{\alpha}{\lambda^2} = 0.0001^2,
\end{align*}

</div>
<p class="fragment">
and thus,
</p>
<div class="fragment">
\begin{align*}
  \alpha = 4, \quad \lambda = 20,000.
\end{align*}

</div>
</section>
</section>
<section>
<section id="slide-org7fc3e18">
<h4 id="org7fc3e18">Example 7.2.4 - Continued</h4>
<p>
So the prior p.d.f. of \(\theta\) is
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta) = \begin{cases}
\frac{20,000^4}{3!} \theta^3 e^{-20,000 \theta}, & \theta>0,\\
0, & \theta \le 0.
\end{cases}
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org8bf4b74">
<h4 id="org8bf4b74">The Posterior Distribution</h4>
<p>
Suppose in Example 7.2.4 that we observe a collection of \(n\) lifetimes of fluorescent lamps, how would we change the distribution of \(\theta\) to take account of the observed data?
</p>

</section>
</section>
<section>
<section id="slide-org7dedbbd">
<h4 id="org7dedbbd">Definition</h4>
<p>
Consider a statistical inference problem with parameter \(\theta\) and random variables \(X_1, \dots, X_n\) to be observed. The conditional distribution of \(\theta\) given \(X_1, \dots, X_n\) is called the <i>posterior distribution</i> of \(\theta\).
</p>

<p class="fragment">
The conditional p.m.f or p.d.f. of \(\theta\) given \(X_1 = x_1, \dots, X_n = x_n\) is typically denoted \(\xi(\theta|x_1, \dots, x_n)\) or \(\xi(\theta|\vec{x}), \vec{x} = (x_1, \dots, x_n)\).
</p>

<p class="fragment">
<span style="color: rgb(24,116,205)">Question</span>: How to find the posterior distribution?
</p>
<p class="fragment">
Answer: Bayes&rsquo; theorem.
</p>

</section>
</section>
<section>
<section id="slide-orgda910b5">
<h4 id="orgda910b5">Theorem 7.2.1</h4>
<p>
Suppose that the \(n\) random variables \(X_1, \dots, X_n\) form a random sample from a distribution for which the p.d.f. or the p.m.f. is \(f(x|\theta)\). Suppose also that the value of the parameter \(\theta\) is unknown and the prior p.d.f. or p.m.f. of \(\theta\) is \(\xi(\theta)\).
</p>
<p class="fragment">
Then the posterior p.d.f. or p.m.f. of \(\theta\) is
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta|\vec{x}) = \frac{f_n(\vec{x}|\theta)\xi(\theta)}{g_n(\vec{x})},\quad \text{for all} ~ \theta\in \Omega,
\end{align*}

</div>
<p class="fragment">
where
</p>
<div class="fragment">
\begin{align*}
  f_n(\vec{x}|\theta) & = f(x_1|\theta)\cdots f(x_n|\theta),\\
  g_n(\vec{x}) & =  \int_\Omega f_n(\vec{x}|\theta) \xi(\theta)\, d\theta.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org61452d1">
<h4 id="org61452d1">Example 7.2.6</h4>
<p>
Lifetimes of Fluorescent Lamps (Revisited)
</p>
<p class="fragment">
Suppose also that the lifetimes \(X_1,\dots, X_n\) of a random sample of \(n\) lamps are observed.
</p>

<p class="fragment">
We shall determine the posterior p.d.f. of \(\theta\) given that \(X_1 = x_1, \dots , X_n = x_n\).
</p>

<p class="fragment">
<span style="color: rgb(24,116,205)">Have</span>: \(X_i\sim \text{Exp} (\theta)\), and the prior p.d.f. \(\xi(\theta)\) of \(\theta\), i.e.,
</p>
<div class="fragment">
\begin{align*}
f(x|\theta) = \begin{cases}
\theta e^{-\theta x}, & x > 0,\\
0, & x \le 0.
\end{cases} \quad
  \xi(\theta) = \begin{cases}
\frac{20,000^4}{3!} \theta^3 e^{-20,000 \theta}, & \theta>0,\\
0, & \theta \le 0.
\end{cases}
\end{align*}

</div>
<p class="fragment">
<span style="color: rgb(24,116,205)">Want</span>: the posterior p.d.f. \(\xi(\theta|\vec{x})\).
</p>


<p class="fragment">
<span style="color: rgb(24,116,205)">Need</span>: \(f_n(\vec{x}|\theta)\) and \(g_n(\vec{x})\).
</p>

</section>
</section>
<section>
<section id="slide-orgb1bcb8b">
<h4 id="orgb1bcb8b">Example 7.2.6 - Continued</h4>
<p>
The conditional joint p.d.f. of \(X_1, \dots, X_n\) can be written in the following form, for \(x_i>0\, (i=1, \dots, n)\):
</p>
<div class="fragment">
\begin{align*}
  f_n(\vec{x}|\theta) = \prod_{i=1}^n \theta e^{-\theta x_i} = \theta^n e^{-\theta y},
\end{align*}

</div>
<p class="fragment">
where \(y = \sum_{i=1}^n x_i\).
</p>

<p class="fragment">
The \(n \text{-dimensional}\) marginal joint p.d.f. of \(X_1, \dots, X_n\) is
</p>
<div class="fragment">
\begin{align*}
  g_n(\vec{x}) & = \int_0^{\infty} \theta^n e^{-\theta y} \frac{20,000^4}{3!} \theta^3 e^{-20,000 \theta}\, d\theta \\
& = C  \int_0^{\infty} \theta^{n+3} e^{-(y+20,000)\theta} \, d\theta \\
& = \frac{C \Gamma(n+4)}{(y+20,000)^{n+4}}.
\end{align*}

</div>
</section>
</section>
<section>
<section id="slide-org59e5840">
<h4 id="org59e5840">Example 7.2.6 - Continued</h4>
<p>
Finally,
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta|\vec{x}) & = \frac{f_n(\vec{x}|\theta)\xi(\theta)}{g_n(\vec{x})}\\
& = \frac{\theta^n e^{-\theta y} \cdot C \theta^3 e^{-20,000 \theta}}{\frac{C \Gamma(n+4)}{(y+20,000)^{n+4}}}\\
& = \frac{(y+20,000)^{n+4} \theta^{n+3}}{\Gamma(n+4)} e^{-(y+20,000)\theta},
\end{align*}

</div>
<p class="fragment">
for \(\theta>0\).
</p>

<p class="fragment">
In other words, \(\xi(\theta|\vec{x})\sim \text{Gamma} (n+4, y+20,000)\).
</p>

</section>
</section>
<section>
<section id="slide-orgb80890d">
<h4 id="orgb80890d">Example 7.2.6 - Continued</h4>
<p>
For example, suppose that we observe the following \(n=5\) lifetimes in hours: \(2911, 3403, 3237, 3509\) and \(3118\). Then \(y = 16,178\), and the posterior p.d.f. of \(\theta\) is the gamma distribution with parameters \(9\) and \(36,178\).
</p>


<div id="orgb567d86" class="figure">
<p><img src="../img/math4750/fig7-1.png" alt="fig7-1.png" class="fragment middle" width="75%" />
</p>
</div>

</section>
</section>
<section>
<section id="slide-org852cc5e">
<h4 id="org852cc5e">Example (Exercise 7.2.3)</h4>
<p>
Suppose that the number of defects on a roll of magnetic recording tape has a Poisson distribution for which the mean \(\lambda\) is either \(1.0\) or \(1.5\), and the prior p.m.f. of \(\lambda\) is as follows:
</p>
<div class="fragment">
\begin{align*}
  \xi(1.0) = 0.4, \quad \xi(1.5) = 0.6.
\end{align*}

</div>

<p class="fragment">
If a roll of tape selected at random is found to have three defects, what is the posterior p.m.f. of \(\lambda\)?
</p>

<p class="fragment">
<i>Solution</i>
</p>
<p class="fragment">
Let \(X\) be the number of defects, and observed \(x = 3\).
</p>
<p class="fragment">
The (conditional) p.m.f. of \(X\):
</p>
<div class="fragment">
\begin{align*}
  f(x|\theta) = \frac{e^{-\theta} \theta^x}{x!}, \quad \text{for } x>0.
\end{align*}

</div>
<p class="fragment">
<span style="color: rgb(24,116,205)">Want</span>: \(\xi(\theta|x)\)?
</p>

</section>
</section>
<section>
<section id="slide-org9ec0c88">
<h4 id="org9ec0c88">Example (Exercise 7.2.3) - Continued</h4>
<p>
Note here we only one sample, that is, \(n=1\).
</p>

<p class="fragment">
The marginal p.m.f. of \(X\) is
</p>
<div class="fragment">
\begin{align*}
  g_1(x) & = f(x|\theta = 1.0) \xi(1.0) + f(x|\theta =1.5)\xi(1.5)\\
& = \frac{e^{-1.0}\cdot 1.0^x}{x!}\cdot 0.4 + \frac{e^{-1.5}\cdot 1.5^x}{x!}\cdot 0.6,
\end{align*}

</div>
<p class="fragment">
and thus \(g_1(3) = 0.09983\dots\)
</p>

</section>
</section>
<section>
<section id="slide-org4275595">
<h4 id="org4275595">Example (Exercise 7.2.3) - Continued</h4>
<p>
Therefore,
</p>
<div class="fragment">
\begin{align*}
  \xi(1.0|3) = \frac{f_1(3|1.0)\xi(1.0)}{g_1(3)} = \frac{\frac{e^{-1.0}\cdot 1.0^3}{3!}\cdot 0.4}{0.09983} = 0.2456,
\end{align*}

</div>
<p class="fragment">
and
</p>
<div class="fragment">
\begin{align*}
  \xi(1.5|3) = 1 - \xi(1.0|3) = 0.7544.
\end{align*}

</div>

<p class="fragment">
Try \(X = 0\)?
</p>

</section>
<section>
<p>
What if \(n>1\)?
</p>

<p class="fragment">
Say we observed \(X_1 = x_1, X_2 = x_2\). Then
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta|x_1, x_2) & = \frac{f_2(x_1, x_2|\theta)\xi(\theta)}{g_2(x_1, x_2)}\\
& = \frac{f(x_2|\theta)f(x_1|\theta)\xi(\theta)}{g_2(x_1, x_2)}\\
& \propto f(x_2|\theta) \xi(\theta|x_1),
\end{align*}

</div>
<p class="fragment">
where \(\propto\) means &ldquo;proportional to (up to a constant, but nothing to do with \(\theta\))&rdquo;.
</p>

</section>
</section>
<section>
<section id="slide-orge7837cc">
<h4 id="orge7837cc">Example 7.2.7</h4>
<p>
Proportion of Defective Items (Revisited)
</p>
<ul>
<li data-fragment-index="1" class="fragment">Observations: \(X_1 = x_1, \dots, X_n = x_n\).</li>
<li data-fragment-index="2" class="fragment">\(X_i \sim \text{Ber} (\theta)\), where \(\theta\) is the (unknown) proportion of defective items.</li>
<li data-fragment-index="3" class="fragment">The prior distribution of \(\theta\): \(\xi(\theta) \sim \text{Unif} ([0,1])\).</li>

</ul>

<p class="fragment">
What is the posterior distribution?
</p>
</section>
</section>
<section>
<section id="slide-org827b8b2">
<h4 id="org827b8b2">Example 7.2.7 - Continued</h4>
<p>
We first re-write the p.m.f. of \(X_i\) as follows:
</p>
<div class="fragment">
\begin{align*}
  f(x|\theta) = \begin{cases}
\theta^x (1 - \theta)^{1-x}, & 0< \theta < 1,\\
0, & \text{otherwise}.
\end{cases}
\end{align*}

</div>
<p class="fragment">
Then
</p>
<div class="fragment">
\begin{align*}
  f_n(\vec{x}|\theta) & = \prod_{i=1}^n f(x_i|\theta) = \prod_{i=1}^n \theta^{x_i}(1- \theta)^{1-x_i}\\
& = \theta^y (1- \theta)^{n-y}, \quad y = \sum_{i=1}^n x_i.
\end{align*}

</div>

<div class="fragment">
\begin{align*}
  g_n(\vec{x}) & = \int_0^1 f_n(\vec{x}|\theta) \xi(\theta)\, d\theta = \int_0^1 \theta^y (1 - \theta)^{n-y}\cdot 1 \, d\theta\\
& = B(y+1, n-y+1).
\end{align*}

</div>
</section>
</section>
<section>
<section id="slide-org4caff17">
<h4 id="org4caff17">Example 7.2.7 - Continued</h4>
<p>
Therefore,
</p>
<div>
\begin{align*}
  \xi(\theta|\vec{x})
& = \frac{f_n(\vec{x}|\theta)\xi(\theta)}{g_n(\vec{x})} = \frac{\theta^y (1 - \theta)^{n-y}}{B(y+1, n-y+1)}\\
& \sim \text{Beta} (y + 1, n - y +1).
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org1b5a47d">
<h4 id="org1b5a47d">Remark</h4>
<p class="fragment">
(1) Recall the p.d.f. of \(\text{Beta} (\alpha, \beta)\): for \(x\in (0,1)\)
</p>
<div class="fragment">
\begin{align*}
    f(x| \alpha, \beta) =
    \frac{x^{\alpha -1}(1 - x)^{\beta -1}}{B(\alpha, \beta)}.
\end{align*}

</div>

<p class="fragment">
If \(\alpha =1, \beta = 1\), then for \(x\in (0,1)\),
</p>
<div class="fragment">
\begin{align*}
  f(x|1, 1) = 1.
\end{align*}

</div>
<p class="fragment">
Thus, \(\text{Beta}(1, 1) = \text{Unif}([0,1])\).
</p>

</section>
<section>
<p>
(2) Sequential observations
</p>
<div class="fragment">
\begin{align*}
  \text{Beta} (1, 1) & \xrightarrow{x_1} ~ \text{Beta} (x_1+1, 1 - x_1 + 1)\\
& \xrightarrow{x_2} ~ \text{Beta} (x_1+x_2+1, 2 - x_1-x_2 + 1)\\
& \quad \vdots\\
& \xrightarrow{x_n} ~ \text{Beta} (y+1, n-y + 1).
\end{align*}

</div>

</section>
<section>
<p>
(3) When the joint p.d.f. or p.m.f. \(f_n(\vec{x}|\theta)\) of the observations in a random sample is regarded as a function of \(\theta\) fro given values of \(x_1, \dots, x_n\), it is called the <i>likelihood function</i>.
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta|\vec{x}) = \frac{f_n(\vec{x}|\theta)\xi(\theta)}{g_n(\vec{x})} \propto f_n(\vec{x}|\theta)\xi(\theta).
\end{align*}

</div>
<p class="fragment">
Thus, the posterior is proportional to the product of the likelihood function and the prior.
</p>

</section>
</section>
<section>
<section id="slide-org0d54296">
<h3 id="org0d54296">7.3 Conjugate Prior Distributions</h3>
<p>
For each of the most popular statistical models, there exists a family of distributions for the parameter with a very special property. If the prior distribution is chosen to be a member of that family, then the posterior distribution will also be a member of that family. Such a family of distributions is called a <i>conjugate</i> family. Choosing a prior distribution from a conjugate family will typically make it particularly simple to calculate the posterior distribution.
</p>

</section>
</section>
<section>
<section id="slide-org61df1e2">
<h4 id="org61df1e2">Sampling from a Bernoulli Distribution</h4>
<p>
<span style="color: rgb(24,116,205)">Theorem 7.3.1</span>
Suppose that \(X_1, \dots , X_n\) form a random sample from the Bernoulli distribution with parameter \(\theta\), which is unknown (\(0 < \theta < 1\)). Suppose also that the prior distribution of \(\theta\) is the beta distribution with parameters \(\alpha>0\) and \(\beta>0\).
</p>
<p class="fragment">
Then the posterior distribution of \(\theta\) given that \(X_i =x_i \, (i=1,\dots,n)\) is the beta distribution with parameters \(\alpha + \sum_{i=1}^n x_i\) and \(\beta + n - \sum_{i=1}^n x_i\).
</p>

</section>
</section>
<section>
<section id="slide-orge01fdc5">
<h4 id="orge01fdc5">Sampling from a Bernoulli Distribution - Continued</h4>
<p>
Proof.
</p>
<p class="fragment">
Prior p.d.f. of \(\theta\): \(\xi(\theta) = C \theta^{\alpha -1} ( 1- \theta )^{\beta-1}, \quad 0<\theta< 0\).
</p>

<p class="fragment">
The joint p.m.f. of \(X_1, \dots, X_n\):
</p>
<div class="fragment">
\begin{align*}
  f_n(\vec{x}|\theta) = \theta^y (1 - \theta)^{n-y}, \quad y =  \sum_{i=1}^n x_i.
\end{align*}

</div>
<p class="fragment">
The posterior distribution of \(\theta\):
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta|\vec{x}) & \propto f_n(\vec{x}|\theta) \xi(\theta) \propto \theta^y(1 - \theta)^{n-y}\cdot \theta^{\alpha -1}(1 - \theta)^{\beta - 1} \propto \theta^{\alpha + y -1}(1 - \theta)^{n + \beta - y - 1}.
\end{align*}

</div>
<p class="fragment">
Therefore, \(\xi(\theta|\vec{x}) \sim \text{Beta} (\alpha + y, n + \beta - y)\).
</p>

</section>
</section>
<section>
<section id="slide-org4ba2fdb">
<h4 id="org4ba2fdb">Remark</h4>
<p class="fragment">
The family of beta distributions is called a <i>conjugate</i> family of prior distribution, for samples from a Bernoulli distribution.
</p>

<p class="fragment">
The family of beta distribution is <i>closed</i> under sampling from a Bernoulli distribution.
</p>

<div class="fragment">
\begin{align*}
  \text{Beta} (\alpha, \beta) \xrightarrow{\text{Ber}(\theta)} \text{Beta} (\alpha + y, \beta + n - y).
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org53e1306">
<h4 id="org53e1306">Sampling from a Poisson Distribution</h4>
<p>
<span style="color: rgb(24,116,205)">Theorem 7.3.2</span>
Suppose that \(X_1, \dots , X_n\) form a random sample from the Poisson distribution with parameter \(\theta>0\), which is unknown. Suppose also that the prior distribution of \(\theta\) is the gamma distribution with parameters \(\alpha>0\) and \(\lambda>0\).
</p>
<p class="fragment">
Then the posterior distribution of \(\theta\) given that \(X_i =x_i \, (i=1,\dots,n)\) is the gamma distribution with parameters \(\alpha + \sum_{i=1}^n x_i\) and \(\lambda + n\).
</p>
<div class="fragment">
\begin{align*}
  \text{Gamma} (\alpha, \lambda) \xrightarrow{\text{Poisson}(\theta)} \text{Gamma} (\alpha + y, \lambda + n).
\end{align*}

</div>

<p class="fragment">
For the proof, similar to Theorem 7.3.1, also check the textbook.
</p>


</section>
</section>
<section>
<section id="slide-org96a244e">
<h4 id="org96a244e">Sampling from a Normal Distribution</h4>
<p>
<span style="color: rgb(24,116,205)">Theorem 7.3.3</span>
Suppose that \(X_1, \dots , X_n\) form a random sample from a normal distribution for which the value of the mean \(\theta\) is unknown and the value of the variance \(\sigma^2>0\) is known. Suppose also that the prior distribution of \(\theta\) is the normal distribution with mean \(\mu_0\) and variance \(v_0^2\).
</p>

<p class="fragment">
Then the posterior distribution of \(\theta\) given that \(X_i =x_i \, (i=1,\dots,n)\) is the normal distribution with \(\mu_1\) and variance \(v_1^2\), where
</p>
<div class="fragment">
\begin{align*}
  \mu_1 & = \frac{\sigma^2 \mu_0 + n v^2_0 \overline{x}_n}{\sigma^2 + n v_0^2}\\
   v_1^2 & = \frac{\sigma^2 v_0^2}{\sigma^2 + n v_0^2}.
\end{align*}

</div>

<div class="fragment">
 \begin{align*}
  \mathcal{N}(\mu_0, v_0^2) \xrightarrow{\mathcal{N}(\theta, \sigma^2)} \mathcal{N}(\mu_1, v_1^2).
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org8c36d51">
<h4 id="org8c36d51">Sampling from a Normal Distribution - Continued</h4>
<p>
Proof.
</p>
<p class="fragment">
The prior:
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta) = \frac{1}{\sqrt{2\pi v_0^2}} e^{-\frac{(\theta - \mu_0)^2}{2 v_0^2}} \propto e^{-\frac{(\theta - \mu_0)^2}{2 v_0^2}}.
\end{align*}

</div>

<p class="fragment">
The likelihood function:
</p>
<div class="fragment">
\begin{align*}
  f_n(\vec{x}|\theta) = \prod_{i=1}^n f(x_i|\theta) \propto e^{-\frac{\sum_{i=1}^n  (x_i - \theta)^2}{2\sigma^2}}.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org7fe2822">
<h4 id="org7fe2822">Sampling from a Normal Distribution - Continued</h4>
<p>
Next, we use the following fact (see Exercise 24 on page 316):
</p>
<div class="fragment">
\begin{align*}
  \sum_{i=1}^n  (x_i - \theta)^2 = n (\theta - \overline{x}_n)^2 + \sum_{i=1}^n (x_i - \overline{x}_n)^2,
\end{align*}

</div>
<p class="fragment">
and so
</p>
<div class="fragment">
\begin{align*}
  f_n(\vec{x}|\theta) \propto e^{-\frac{n(\theta - \overline{x}_n)^2}{2\sigma^2}}.
\end{align*}

</div>
<p class="fragment">
Thus, the posterior:
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta|\vec{x}) & \propto f_n(\vec{x}|\theta) \xi(\theta) \\
& \propto e^{-\frac{n(\theta - \overline{x}_n)^2}{2\sigma^2}}\cdot e^{-\frac{(\theta - \mu_0)^2}{2 v_0^2}}\\
& \propto  e^{-\frac{(\theta- \mu_1)^2}{2v_1^2}}.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org9e0123b">
<h4 id="org9e0123b">Sampling from an Exponential Distribution</h4>
<p>
<span style="color: rgb(24,116,205)">Theorem 7.3.4</span>
Suppose that \(X_1, \dots , X_n\) form a random sample from the exponential distribution with parameter \(\theta>0\), which is unknown. Suppose also that the prior distribution of \(\theta\) is the gamma distribution with parameters \(\alpha>0\) and \(\lambda>0\).
</p>

<p class="fragment">
Then the posterior distribution of \(\theta\) given that \(X_i =x_i \, (i=1,\dots,n)\) is the gamma distribution with parameters \(\alpha + n\) and \(\lambda + \sum_{i=1}^n x_i\).
</p>
<div class="fragment">
\begin{align*}
  \text{Gamma} (\alpha, \lambda) \xrightarrow{\text{Exp}(\theta)} \text{Gamma} (\alpha + n, \lambda + \sum_{i=1}^n x_i).
\end{align*}

</div>

<p class="fragment">
For the proof, check the textbook.
</p>

</section>
</section>
<section>
<section id="slide-org2b46346">
<h3 id="org2b46346">7.4 Bayes Estimators</h3>
<div class="outline-text-3" id="text-org2b46346">
</div>
</section>
</section>
<section>
<section id="slide-org84a3509">
<h4 id="org84a3509">Example</h4>
<p>
Assume in a (giant) box, there are either red or blue balls. You might wish to estimate the (true) proportion \(\theta\) of red balls without specifying the entire distribution.
</p>
<p class="fragment">
<span style="color: rgb(24,116,205)">Question</span>: How to choose such a single-number estimate?
</p>

</section>
</section>
<section>
<section id="slide-orgd16acf0">
<h4 id="orgd16acf0">Definition</h4>
<p>
Let \(X_1, \dots X_n\) form a random sample from a distribution with p.d.f. or p.m.f. \(f(x|\theta)\), where \(\theta\) is unknown.
</p>

<p class="fragment">
An <i>estimator</i> of the parameter \(\theta\), based on the random sample \(X_1, \dots, X_n\), is a real-valued function \(\delta(X_1, \dots, X_n)\).
</p>

<p class="fragment">
If \(X_1 = x_1, \dots, X_n = x_n\) are observed, then \(\delta(x_1, \dots, x_n)\) is called the <i>estimate</i> of \(\theta\).
</p>

</section>
</section>
<section>
<section id="slide-org6c94b58">
<h4 id="org6c94b58">Example</h4>
<p>
Let \(X_1, \dots, X_n\) be a random sample such that
</p>
<div>
\begin{align*}
  X_i = \begin{cases}
1, & \mbox{if $i\text{-th}$ ball is red}, \\
0, & \mbox{if $i\text{-th}$ ball is blue}.
\end{cases}
\end{align*}

</div>
<p class="fragment">
Then the sample mean
</p>
<div class="fragment">
\begin{align*}
  \overline{X}_n = \frac{\sum_{i=1}^n X_i}{n}
\end{align*}

</div>
<p class="fragment">
is an estimator of the proportion \(\theta\).
</p>

</section>
</section>
<section>
<section id="slide-org38f2c12">
<h4 id="org38f2c12">Example - Continued</h4>
<p>
If \(X_1 = 1, X_2=1, X_3 = 0\) are observed, then
</p>
<div class="fragment">
\begin{align*}
  \overline{x}_3 = \frac{1 + 1 + 0}{3} = \frac{2}{3}
\end{align*}

</div>
<p class="fragment">
is an estimate of \(\theta\).
</p>

<p class="fragment">
<span style="color: rgb(24,116,205)">Note</span>: The estimator \(\delta(X_1,\dots, X_n)\) is a random variable, but the estimate \(\delta(x_1, \dots, x_n)\) is a real value.
</p>

<p class="fragment">
<span style="color: rgb(24,116,205)">Question:</span> How good is the estimator/estimate?
</p>

</section>
</section>
<section>
<section id="slide-orge91cda6">
<h4 id="orge91cda6">Definition (Loss Function)</h4>
<p>
A <i>loss function</i> is a real-valued function of two variables, \(L(\theta, a)\), where \(\theta\in \Omega\), and \(a\) is a real number.
</p>

<p class="fragment">
The interpretation is that the statistician loses \(L(\theta, a)\) if the parameter equals \(\theta\), and the estimate equals \(a\).
</p>

</section>
</section>
<section>
<section id="slide-org0c48acf">
<h4 id="org0c48acf">Different Loss Functions</h4>
<ul>
<li>The <i>absolute error loss</i>: \(L(\theta, a) = |\theta - a|\)</li>
<li>The <i>square error loss</i>: \(L(\theta, a) = (\theta - a)^2\).</li>

</ul>

<p class="fragment">
<span style="color: rgb(24,116,205)">Question:</span>
If we would like to find the best estimate, then what do we mean by the &ldquo;best&rdquo;?
</p>

</section>
</section>
<section>
<section id="slide-orgb27bf9b">
<h4 id="orgb27bf9b">Bayes Estimator</h4>
<p class="fragment">
Let \(\vec{x} = (x_1, \dots, x_n)\) be the observed values, and \(\xi(\theta|\vec{x})\) be the posterior distribution of \(\theta\). Then the (conditional) <i>expected loss</i> is
</p>
<div class="fragment">
\begin{align}
    \label{eq:7-4-2}
    \mathbb{E}(L(\theta, a)|\vec{x}) = \int_{\Omega} L(\theta, a) \xi(\theta | \vec{x})\, d\theta\tag{7.4.2}
\end{align}

</div>

<p class="fragment">
We would like to choose an \(a\) which minimizes \eqref{eq:7-4-2}.
</p>
<p class="fragment">
Clearly, the value of \(a\) depends on the observations \(\vec{x}\).
</p>

</section>
</section>
<section>
<section id="slide-org5bb43fa">
<h4 id="org5bb43fa">Bayes Estimator - Continued</h4>
<p>
We call \(\delta^\ast(\vec{x})\), a value of \(a\) such that \(\mathbb{E}(L(\theta| a)|\vec{x})\) is minimized, a <i>Bayes estimate</i> of \(\theta\), and \(\delta^\ast(\vec{X})\), a <i>Bayes estimator</i> of \(\theta\).
</p>

<p class="fragment">
That is, for any \(\vec{x}\), we have
</p>
<div class="fragment">
\begin{align*}
  \mathbb{E}(L(\theta, \delta^\ast(\vec{x}))|\vec{x}) = \min_{a\in \Omega} \mathbb{E}(L(\theta, a)| \vec{x}).
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-orgf94a073">
<h4 id="orgf94a073">Estimate the Parameter of a Bernoulli Distribution</h4>
<p class="fragment">
Assume random sample \(X_1, \dots, X_n\) such that \(X_i\sim \text{Ber}(\theta)\)
</p>
<p class="fragment">
Prior distribution of \(\theta\): \(\xi(\theta)\sim \text{Beta}(5, 10)\).
</p>

<p class="fragment">
Let \(Y = \sum_{i=1}^n X_i\), and observe \(y = 1, n = 20\).
</p>

<p class="fragment">
Find the Bayes estimate of \(\theta\) using the square error loss function, i.e., to find a value of \(a\) which minimizes
</p>
<div class="fragment">
\begin{align*}
  \mathbb{E}((\theta - a)^2 | \vec{x}).
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org40d0ce0">
<h4 id="org40d0ce0">Estimate the Parameter of a Bernoulli Distribution - Continued</h4>
<p>
<span style="color: rgb(24,116,205)">Fact:</span> (check Theorem 4.7.3 on page 260-261)
</p>
<p class="fragment">
Suppose that the square error loss function is used and that the posterior mean of \(\theta\), \(\mathbb{E}(\theta|\vec{X})\), is finite. Then, a Bayes estimator of \(\theta\) is \(\delta^\ast(\vec{X}) = \mathbb{E}(\theta|\vec{X})\).
</p>

<p class="fragment">
Back to the previous example.
</p>
<p class="fragment">
By Theorem 7.3.1 (page 394 - 395),
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta | \vec{x}) \sim \text{Beta}(\alpha + y, n - y + \beta) = \text{Beta} (6, 29).
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org27cea8d">
<h4 id="org27cea8d">Estimate the Parameter of a Bernoulli Distribution - Continued</h4>
<p>
Recall that
</p>
<div class="fragment">
\begin{align*}
  \mathbb{E}(X) = \frac{\alpha}{\alpha + \beta}\quad \text{if}~X \sim \text{Beta} (\alpha, \beta).
\end{align*}

</div>
<p class="fragment">
Therefore,
</p>
<div class="fragment">
\begin{align*}
  \delta^\ast(\vec{x}) = \mathbb{E}(\theta|\vec{x}) = \frac{6}{6+29} = \frac{6}{35}.
\end{align*}

</div>

<p class="fragment">
In general, a Bayes estimator in this case is
</p>
<div class="fragment">
\begin{align*}
  \delta^\ast(\vec{x}) = \mathbb{E}(\theta|\vec{x}) = \frac{\alpha + Y}{\alpha + \beta + n}.
\end{align*}

</div>


</section>
</section>
<section>
<section id="slide-org22f3c69">
<h4 id="org22f3c69">Estimate the Parameter of a Poisson Distribution</h4>
<p class="fragment">
Assume random sample \(X_1, \dots, X_n\) such that \(X_i\sim \text{Poisson}(\theta)\).
</p>
<p class="fragment">
Prior distribution of \(\theta\): \(\xi(\theta)\sim \text{Gamma}(3, 1)\).
</p>

<p class="fragment">
Observed:
</p>
<div class="fragment">
\begin{align*}
  (X_1, \dots, X_5) = (2, 2, 6, 0, 3).
\end{align*}

</div>

<p class="fragment">
Find the Bayes estimate of \(\theta\) using the square error loss function.
</p>

</section>
</section>
<section>
<section id="slide-org9237b2a">
<h4 id="org9237b2a">Estimate the Parameter of a Poisson Distribution - Continued</h4>
<p class="fragment">
By Theorem 7.3.2 (page 397),
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta | \vec{x}) \sim \text{Gamma}(\alpha + y, \lambda + n) = \text{Gamma} (16, 6).
\end{align*}

</div>

<p class="fragment">
Recall that
</p>
<div class="fragment">
\begin{align*}
  \mathbb{E}(X) = \frac{\alpha}{\lambda}\quad \text{if}~X \sim \text{Gamma} (\alpha, \lambda).
\end{align*}

</div>
<p class="fragment">
Therefore,
</p>
<div class="fragment">
\begin{align*}
  \delta^\ast(\vec{x}) = \mathbb{E}(\theta|\vec{x}) = \frac{16}{6} = \frac{8}{3}.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org12c9477">
<h4 id="org12c9477">Estimate the Mean of a Normal Distribution</h4>
<p class="fragment">
Assume random sample \(X_1, \dots, X_n\) such that \(X_i\sim \mathcal{N}(\theta, \sigma^2)\), where \(\theta\) is unknown, but \(\sigma^2\) is known.
</p>

<p class="fragment">
Prior distribution of \(\theta\): \(\xi(\theta)\sim \mathcal{N}(\mu_0, v_0^2)\).
</p>

<p class="fragment">
Find the Bayes estimate of \(\theta\) using the square error loss function.
</p>

</section>
</section>
<section>
<section id="slide-orgd8f10ec">
<h4 id="orgd8f10ec">Estimate the Mean of a Normal Distribution - Continued</h4>
<p class="fragment">
By Theorem 7.3.3 (page 398),
</p>
<div class="fragment">
\begin{align*}
  \xi(\theta | \vec{x}) \sim \mathcal{N}(\mu_1, v_1^2).
\end{align*}

</div>

<p class="fragment">
Therefore,
</p>
<div class="fragment">
\begin{align*}
  \delta^\ast(\vec{x}) = \mathbb{E}(\theta|\vec{x}) = \mu_1 =\frac{\sigma^2 \mu_0 + n v^2_0 \overline{x}_n}{\sigma^2 + n v_0^2}.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org55e2173">
<h4 id="org55e2173">What if we choose the absolute error loss function?</h4>
<p class="fragment">
<span style="color: rgb(24,116,205)">Fact:</span>
</p>
<p class="fragment">
When the absolute error loss function is used, a Bayes estimator is \(\delta^\ast(\vec{X})\) equal to to a <b>median</b> of the posterior distribution of \(\theta\).
</p>

<p class="fragment">
For normal distribution, since the mean is the same as the median, \(\delta^\ast(\vec{X})\) is the same for both \(L(\theta, a) = (\theta - a)^2\), and \(L(\theta, a) = |\theta - a|\).
</p>

</section>
</section>
<section>
<section id="slide-orgea31ef0">
<h4 id="orgea31ef0">Consistency of the Bayes Estimator</h4>
<p class="fragment">
If \(\delta^\ast(\vec{X}) \to \theta\) as \(n \to \infty\), the sequence of estimators \(\delta^\ast(\vec{X})\) is called a <i>consistent</i> sequence of Bayes estimators.
</p>

<p class="fragment">
In most cases, the Bayes estimators form a consistence sequence.
</p>
</section>
</section>
</div>
</div>
<script src="../dist/reveal.js"></script>
<script src="../plugin/markdown/markdown.js"></script>
<script src="../plugin/notes/notes.js"></script>
<script src="../plugin/search/search.js"></script>
<script src="../plugin/zoom/zoom.js"></script>
<script src="../plugin/reveal.js-menu/menu.js"></script>
<script src="../reveal.js-plugins/chalkboard/plugin.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: false,
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,
overview: true,

transition: 'none',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealMarkdown, RevealNotes, RevealSearch, RevealZoom, RevealMenu, RevealChalkboard ],

// Optional libraries used to extend reveal.js
dependencies: [
]

,chalkboard: {src: "chalkboard/chalkboard.json", storage: "chalkboard-demo", toggleChalkboardButton: { left: "80px" },	toggleNotesButton: { left: "130px" },	colorButtons: 5}});
</script>
</body>
</html>

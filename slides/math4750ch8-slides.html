<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="utf-8"/>
<title>MATH 4750 - Introduction to Mathematical Statistics</title>
<meta name="author" content="\\
Jie Zhong \\
Department of Mathematics \\
California State University, Los Angeles"/>
<style type="text/css">
.underline { text-decoration: underline; }
</style>
<link rel="stylesheet" href="../dist/reveal.css"/>

<link rel="stylesheet" href="../dist/theme/serif.css" id="theme"/>

<link rel="stylesheet" href="../reveal.js-plugins/chalkboard/style.css"/>

<link rel="stylesheet" href="../reveal.js-plugins/menu/font-awesome/css/fontawesome.css"/>

<link rel="stylesheet" href="../gnohz.css"/>
<script>window.MathJax = { TeX: {Macros: {range: "\\text{Range}", ow: "\\text{otherwise}"}} }</script>
<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</head>
<body>
<div class="reveal">
<div class="slides">
<section id="sec-title-slide">
<h1>MATH 4750 - Introduction to Mathematical Statistics</h1><h2></h2><h6> <br />
Jie Zhong <br />
Department of Mathematics <br />
California State University, Los Angeles</h6>
</section>

<section>
<section id="slide-orge3d9f6c">
<h2 id="orge3d9f6c">Chapter 8 Sampling Distributions of Estimators</h2>
<div class="outline-text-2" id="text-orge3d9f6c">
</div>
</section>
</section>
<section>
<section id="slide-orgc26df76">
<h3 id="orgc26df76">8.1 The Sampling Distribution of a Statistic</h3>
<p>
A statistic is a function of some observable random variables, and hence is itself a random variable with a distribution. That distribution is its sampling distribution, and it tells us what values the statistic is likely to assume and how likely it is to assume those values prior to observing our data. When the distribution of the observable data is indexed by a parameter, the sampling distribution is specified as the distribution of the statistic for a given value of the parameter.
</p>

</section>
</section>
<section>
<section id="slide-org22e4bf8">
<h4 id="org22e4bf8">Definition</h4>
<p>
<span style="color: rgb(24,116,205)">Sampling Distribution</span>
</p>
<p class="fragment">
Suppose that the random variables \(\vec{X} = (X_1, \dots, X_n)\) form a random sample from a distribution involving a parameter \(\theta\) whose value is unknown. Let \(T\) be a function of \(\vec{X}\) and possibly \(\theta\). That is, \(T = r(X_, \dots, X_n, \theta)\).
</p>
<p class="fragment">
The distribution of \(T\) (given \(\theta\)) is called the <i>sampling distribution</i> of \(T\).
</p>

<p class="fragment">
<span style="color: rgb(24,116,205)">Note</span>: Often, the random variable \(T\) in the definition here will not depend on \(\theta\), and hence will be a <b>statistic</b> as defined in Sec 7.1.
</p>


</section>
</section>
<section>
<section id="slide-org205d5f0">
<h4 id="org205d5f0">Example 8.1.3</h4>
<p>
<span style="color: rgb(24,116,205)">Lifetimes of Electronic Components</span>
</p>
<p class="fragment">
Consider the company in Example 7.1.1 that sells electronic components.
</p>
<p class="fragment">
Observations: \(X_1, \dots, X_n \sim \text{Exp}(\theta)\).
</p>
<p class="fragment">
Prior distribution: \(\xi(\theta) \sim \text{Gamma}(1, 2)\).
</p>
<p class="fragment">
Posterior distribution: \(\xi(\theta|\vec{X}) \sim \text{Gamma} (1 + 3, 2 + \sum_{i=1}^3 X_i)\) after observing \(n=3\) lifetimes.
</p>

</section>
</section>
<section>
<section id="slide-org572d6cb">
<h4 id="org572d6cb">Example 8.1.3 - Continued</h4>
<p>
Posterior mean:
</p>
<div class="fragment">
\begin{align*}
  \hat{\theta} = \frac{4}{2 + \sum_{i=1}^3 X_i}.
\end{align*}

</div>

<p class="fragment">
Prior to observing the three lifetimes, the company may want to know how likely it is that \(\hat{\theta}\) will be close to \(\theta\).
</p>

<p class="fragment">
For example, they may want to compute
</p>
<div class="fragment">
\begin{align*}
  \mathbb{P}(|\hat{\theta} - \theta|< 0.1).
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org9496c30">
<h4 id="org9496c30">Example 8.1.2</h4>
<p>
<span style="color: rgb(24,116,205)">Sampling Distribution of the M.L.E. of the Mean of a Normal Distribution</span>
</p>
<p class="fragment">
A random sample \(X_1, \dots, X_n\) from \(\mathcal{N}(\mu, \sigma^2)\).
</p>

<p class="fragment">
We already know that the M.L.E. of the mean:
</p>
<div class="fragment">
\begin{align*}
  \hat{\mu} = \overline{X}_n = \frac{X_1 + \cdots + X_n}{n}.
\end{align*}

</div>
<p class="fragment">
Also, we know that (from Sec 5.6)
</p>
<div class="fragment">
\begin{align*}
  \hat{\mu} \sim \mathcal{N}(\mu, \sigma^2/n).
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org778dc3b">
<h4 id="org778dc3b">Example 8.1.2 - Continued</h4>
<p>
But what about the sample variance?
</p>
<div class="fragment">
\begin{align*}
  \widehat{\sigma^2} = \frac{1}{n}\sum_{i=1}^n (X_i - \overline{X}_n)^2,
\end{align*}

</div>
<p class="fragment">
so what is the distribution of \(\widehat{\sigma^2}\)?
</p>

<p class="fragment">
In this chapter, we shall derive, for random samples from a normal distribution, the distribution of the sample variance and the distributions of various functions of the sample mean and the sample variance. These derivations will lead us to the definitions of some new distributions that play important roles in problems of statistical inference.
</p>

</section>
</section>
<section>
<section id="slide-org07110b2">
<h3 id="org07110b2">8.2 The Chi-Square Distributions</h3>
<p>
The family of chi-square (\(\chi^2\)) distributions is a subcollection of the family of gamma distributions. These special gamma distributions arise as sampling distributions of variance estimators based on random samples from a normal distribution.
</p>

</section>
</section>
<section>
<section id="slide-org4524846">
<h4 id="org4524846">Example 8.2.1</h4>
<p>
<span style="color: rgb(24,116,205)">M.L.E. of the Variance of a Normal Distribution</span>
</p>
<p class="fragment">
Suppose a random sample \(X_1, \dots, X_n\) from \(\mathcal{N}(\mu, \sigma^2)\), with known \(\mu\) but unknown \(\sigma^2\).
</p>
<p class="fragment">
Find the M.L.E. of the variance.
</p>
<p class="fragment">
From Exercise 6 in Sec 7.5, we have
</p>
<div class="fragment">
\begin{align*}
  \widehat{\sigma_0^2} = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2.
\end{align*}

</div>
<p class="fragment">
The distribution of \(\widehat{\sigma^2_0}\) and \(\widehat{\sigma^2_0}/\sigma^2\) are useful in several statistical problems, and we will derive them in this section.
</p>

</section>
</section>
<section>
<section id="slide-orgb9c090c">
<h4 id="orgb9c090c">Gamma Distribution (review)</h4>
<p>
Recall that if \(X \sim \text{Gamma} (\alpha, \beta)\):
</p>
<div class="fragment">
\begin{align*}
  f(x|\alpha,\beta ) = \begin{cases}
\frac{1}{\Gamma(\alpha)} \beta^\alpha x^{\alpha -1 } e^{-\beta x}, & x>0\\
0, & x \le 0.
\end{cases}
\end{align*}

</div>
<p class="fragment">
Also,
</p>
<div class="fragment">
\begin{align*}
  \mathbb{E}(X) = \frac{\alpha}{\beta}, \quad \text{Var}(X)  = \frac{\alpha}{\beta^2}.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org3cb4c0c">
<h4 id="org3cb4c0c">Definition</h4>
<p class="fragment">
Given a positive integer \(m\), the gamma distribution with \(\alpha = m/2, \beta = 1/2\), is called the \(\chi^2\) distribution with \(m\) degrees of freedom.
</p>

<p class="fragment">
Then its p.d.f. is
</p>
<div class="fragment">
\begin{align*}
  f(x | m) = \begin{cases}
\frac{1}{2^{m/2}\Gamma(m/2)} x^{m/2 - 1} e^{-x/2}, & x >0\\
0, & x\le 0.
\end{cases}
\end{align*}

</div>
<p class="fragment">
Write \(X \sim \chi^2 (m)\).
</p>

</section>
</section>
<section>
<section id="slide-org3bc6fab">
<h4 id="org3bc6fab">Facts of \(\chi^2\) Distribution</h4>
<p>
We also have
</p>
<div class="fragment">
\begin{align*}
  \mathbb{E}(X) = \frac{\alpha}{\beta} = m, \quad \text{Var}(X)  = \frac{\alpha}{\beta^2} = 2m.
\end{align*}

</div>
<p class="fragment">
The m.g.f. is
</p>
<div class="fragment">
\begin{align*}
  \psi_X(t) = \left( \frac{1/2}{1/2 - t} \right)^{m/2} = \left( \frac{1}{1 - 2t} \right)^{m/2},
\end{align*}

</div>
<p class="fragment">
for \(t < 1/2\).
</p>

</section>
</section>
<section>
<section id="slide-orgc5f679b">
<h4 id="orgc5f679b">Theorem 8.2.2</h4>
<p class="fragment">
If random variables \(X_1, \dots, X_k\) are independent and \(X_i \sim \chi^2(m_i), \, i = 1, \dots, k\), then
</p>
<div class="fragment">
\begin{align*}
  X_1 + \cdots + X_k \sim \chi^2(m_1 + \cdots + m_k).
\end{align*}

</div>

<p class="fragment">
Idea of the proof:
</p>
<div class="fragment">
\begin{align*}
  X_i \sim \text{Gamma}(\alpha_i, 1/2) \Rightarrow  \sum_{i=1}^k X_i \sim \text{Gamma}\left(\sum_{i=1}^k \alpha_i, 1/2\right).
\end{align*}

</div>
</section>
</section>
<section>
<section id="slide-org32f5746">
<h4 id="org32f5746">Theorem 8.2.3 (relation between normal and \(\chi^2\) distributions)</h4>
<p class="fragment">
If \(X\sim \mathcal{N}(0,1)\), then \(X^2 \sim \chi^2(1)\).
</p>

<p class="fragment">
<span style="color: rgb(24,116,205)">Want To Show</span>: \(Y = X^2\) has the p.d.f.
</p>
<div class="fragment">
\begin{align*}
  f_Y(y) = \frac{1}{\sqrt{2}\Gamma(1/2)} y^{-1/2} e^{-y/2}, \, y >0.
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-orgc6c875e">
<h4 id="orgc6c875e">Theorem 8.2.3 - Continued</h4>
<p>
We first find the C.D.F. of \(Y\):
</p>
<div class="fragment">
\begin{align*}
  F_Y(y) & = \mathbb{P}(Y\le y ) = \mathbb{P}(X^2 \le y)\\
& = \mathbb{P}(-y^{1/2} \le X \le y^{1/2})\\
& = \Phi(y^{1/2}) - \Phi(-y^{1/2})\\
& = 2 \Phi(y^{1/2}) -1.
\end{align*}

</div>
<p class="fragment">
Note that
</p>
<div class="fragment">
\begin{align*}
  \frac{d}{dy} \Phi(y^{1/2}) & = \phi(y^{1/2}) \cdot \frac{1}{2} y^{-1/2}\\
& = \frac{1}{\sqrt{2\pi}} e^{-y/2} \cdot \frac{1}{2} y^{-1/2}.
\end{align*}

</div>
</section>
</section>
<section>
<section id="slide-org3cd90c2">
<h4 id="org3cd90c2">Theorem 8.2.3 - Continued</h4>
<p>
So,
</p>
<div class="fragment">
\begin{align*}
  f_Y(y) & = F'_Y(y) = 2 \frac{d}{dy} \Phi(y^{1/2}) \\
& = 2 \frac{1}{\sqrt{2\pi}} e^{-y/2} \cdot \frac{1}{2} y^{-1/2}\\
& = \frac{1}{\sqrt{2}\sqrt{\pi}} y^{-1/2} e^{-y/2}.
\end{align*}

</div>
<p class="fragment">
Since \(\Gamma(1/2) = \sqrt{\pi}\) (see Sec 5.7.9 on page 318), we have \(X^2 \sim \chi^2(1)\).
</p>

</section>
</section>
<section>
<section id="slide-org6f5a7fc">
<h4 id="org6f5a7fc">Corollary 8.2.1</h4>
<p class="fragment">
If the random variables \(X_1, \dots, X_m\) are i.i.d. with the standard normal distribution, then the sum of squares \(X_1^2 + \cdots + X_m^2\) has the \(\chi^2\) distribution with \(m\) degrees of freedom.
</p>

</section>
</section>
<section>
<section id="slide-orgd12f32a">
<h4 id="orgd12f32a">Example 8.2.2</h4>
<p>
<span style="color: rgb(24,116,205)">M.L.E. of the Variance of a Normal Distribution (revisited)</span>
</p>
<p class="fragment">
Suppose a random sample \(X_1, \dots, X_n\) from \(\mathcal{N}(\mu, \sigma^2)\), with known \(\mu\) but unknown \(\sigma^2\).
</p>

<p class="fragment">
The M.L.E. of the variance:
</p>
<div class="fragment">
\begin{align*}
  \widehat{\sigma_0^2} = \frac{1}{n} \sum_{i=1}^n (X_i - \mu)^2.
\end{align*}

</div>

<p class="fragment">
Note that the random variables
</p>
<div class="fragment">
\begin{align*}
    Z_i = \frac{X_i - \mu}{\sigma}, \quad \text{for } i = 1, \dots, n
\end{align*}

</div>
<p class="fragment">
form a random sample from \(\mathcal{N}(0, 1)\).
</p>


</section>
</section>
<section>
<section id="slide-orgb0b8df9">
<h4 id="orgb0b8df9">Example 8.2.2 - Continued</h4>
<p>
By Corollary 8.2.1, we know that \(\sum_{i=1}^n Z_i^2\) has the \(\chi^2\) distribution with \(n\) degrees of freedom.
</p>
<p class="fragment">
<span style="color: rgb(24,116,205)">Question</span>: What the relation between \(\widehat{\sigma^2_0}\) and \(\sum_{i=1}^n Z_i^2\)?
</p>
<div class="fragment">
\begin{align*}
  \frac{n \widehat{\sigma^2_0}}{\sigma^2} = \sum_{i=1}^n Z_i^2.
\end{align*}

</div>

<p class="fragment">
In other words, \(n \widehat{\sigma^2_0}/\sigma^2\)  has the \(\chi^2\) distribution with \(n\) degrees of freedom.
</p>
</section>
</section>
<section>
<section id="slide-org27700de">
<h4 id="org27700de">Exercise 5</h4>
<p>
Suppose that a point \((X, Y, Z)\) is to be chosen at random in three-dimensional space, where \(X\), \(Y\), and \(Z\) are independent random variables and each has the standard normal distribution.
</p>

<p class="fragment">
What is the probability that the distance from the origin to the point will be less than 1 unit?-
</p>

</section>
</section>
<section>
<section id="slide-orged0b7eb">
<h4 id="orged0b7eb">Exercise 5 - Continued</h4>
<p>
Distance: \(\sqrt{X^2 + Y^2 + Z^2}\).
</p>

<p class="fragment">
<span style="color: rgb(24,116,205)">Want</span>:
</p>
<div class="fragment">
\begin{align*}
  \mathbb{P}(\sqrt{X^2 + Y^2 + Z^2} \le 1) & = \mathbb{P}(X^2 + Y^2 + Z^2 \le 1)\\
& \approx 0.2 \quad (\text{see Table on page 858-859})
\end{align*}

</div>

</section>
</section>
<section>
<section id="slide-org723a7c7">
<h4 id="org723a7c7">Exercise 3</h4>
<p>
Sketch the p.d.f. of the \(\chi^2\) distribution with \(m=2\) degrees of freedom.
</p>

<p data-fragment-index="1" class="fragment">
When \(m = 2\), the p.d.f is
</p>
<div data-fragment-index="2" class="fragment">
\begin{align*}
  f(x | m = 2) = \frac{1}{\Gamma(1)} x^0 e^{-x/2} = \frac{1}{2}e^{-x/2}, \, x>0.
\end{align*}

</div>
<ul>
<li data-fragment-index="4" class="fragment">\(\lim_{x\to \infty} f(x) = 0\)</li>
<li data-fragment-index="5" class="fragment">\(\lim_{x\to 0+} f(x) = 1/2 = ~\text{mode}\)</li>
<li data-fragment-index="6" class="fragment">\(f'(x) = - e^{-x/2}/4 < 0\)</li>
<li data-fragment-index="7" class="fragment">\(f''(x) = e^{-x/2}/8 > 0\)</li>

</ul>

<p data-fragment-index="8" class="fragment">
Also, we know the mean is \(2\) and the variance is \(4\).
</p>
</section>
</section>
</div>
</div>
<script src="../dist/reveal.js"></script>
<script src="../plugin/markdown/markdown.js"></script>
<script src="../plugin/notes/notes.js"></script>
<script src="../plugin/search/search.js"></script>
<script src="../plugin/zoom/zoom.js"></script>
<script src="../plugin/reveal.js-menu/menu.js"></script>
<script src="../reveal.js-plugins/chalkboard/plugin.js"></script>
<script>
// Full list of configuration options available here:
// https://github.com/hakimel/reveal.js#configuration
Reveal.initialize({

controls: true,
progress: true,
history: false,
center: true,
slideNumber: false,
rollingLinks: false,
keyboard: true,
mouseWheel: false,
fragmentInURL: false,
hashOneBasedIndex: false,
pdfSeparateFragments: true,
overview: true,

transition: 'none',
transitionSpeed: 'default',

// Plugins with reveal.js 4.x
plugins: [ RevealMarkdown, RevealNotes, RevealSearch, RevealZoom, RevealMenu, RevealChalkboard ],

// Optional libraries used to extend reveal.js
dependencies: [
]

,chalkboard: {src: "chalkboard/chalkboard.json", storage: "chalkboard-demo", toggleChalkboardButton: { left: "80px" },	toggleNotesButton: { left: "130px" },	colorButtons: 5}});
</script>
</body>
</html>
